---
title: "GCP 기반 AI 학습 "
excerpt: "GCP 기반 AI 인재 양성 프로그램에서 다룬 머신러닝 회귀/분류 알고리즘을 학습한 내용을 정리한 포스트입니다."
categories:
  - AI
  - Machine Learning
tags:
  - [AI, GCP, 머신러닝, 회귀, 분류]
toc: true
toc_sticky: true
date: 2024-09-10
last_modified_at: 2024-09-10
---




**회귀/분류 알고리즘**  
    3.1 회귀 분석: Linear Regression (선형 회귀)  
    3.2 다중 회귀 분석: Multiple Linear Regression

---

## 1. 회귀/분류 알고리즘

### 1.1 선형 회귀 (Linear Regression)
- **정의**: 하나 이상의 독립 변수(x)와 종속 변수(y)의 선형 관계를 모델링하는 알고리즘.
  
#### 단순 선형 회귀 분석 (Simple Linear Regression)
- **모델**: 하나의 독립 변수를 사용하여 종속 변수를 예측.
- **수학적 표현**:  
  \( y = W \cdot x + b \)  
  여기서 W는 가중치(weight), b는 편향(bias)를 의미하며, 각각 직선의 기울기와 절편을 의미한다.

- **예시**:
  - 하루에 공부하는 시간이 길어질수록 성적이 오르는 경향이 있음.
  - 집의 평수가 클수록 매매가가 높아지는 경향이 있음.

#### 다중 선형 회귀 분석 (Multiple Linear Regression)
- **정의**: 여러 개의 독립 변수를 사용하여 종속 변수를 예측.
- **예시**: 집의 매매 가격은 평수 외에도 층수, 방의 개수, 지하철과의 거리 등 여러 요인에 영향을 받음.

---

## 2. 머신러닝에서 가설 세우기

- **가설**: 주어진 데이터를 기반으로 입력 변수와 출력 변수의 관계를 나타내는 수식을 설정.
- **예시**: 공부 시간과 성적 데이터를 기반으로 다음과 같은 가설을 세움.  
  \( H(x) = W \cdot x + b \)

---

## 3. 비용 함수 (Cost Function)

- **비용 함수**: 실제값과 예측값 사이의 오차를 측정하는 함수. 머신러닝에서는 이 오차를 최소화하는 W와 b를 찾는 과정이 중요하다.
- **평균 제곱 오차(MSE)**:  
  \( MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - H(x_i))^2 \)

  - **예시 계산**:  
    \( H(x) = 13x + 1 \)  
    실제값과 예측값의 오차를 계산하고, 평균 제곱 오차(MSE)를 통해 W와 b를 조정.

---

## 4. 옵티마이저 (Optimizer) 및 경사 하강법 (Gradient Descent)

- **정의**: 비용 함수의 기울기를 계산하여, 최소값이 되는 방향으로 W와 b를 조정하는 방법.
- **경사 하강법**:  
  \( W := W - \alpha \frac{\partial}{\partial W} \text{cost}(W) \)  
  여기서 \( \alpha \)는 학습률(learning rate)로, 얼마나 큰 폭으로 이동할지를 결정한다.

### 학습률 조정의 중요성
- 학습률이 너무 크면 오차가 발산하여 학습이 불안정.
- 학습률이 너무 작으면 학습 속도가 느려짐.

---

이와 같이 선형 회귀 모델은 회귀 알고리즘의 기초이며, 경사 하강법을 통해 최적의 W와 b 값을 찾아 예측 성능을 향상시킬 수 있습니다.
